{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependences\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splinter method ----this will be added each time A NEW REQUEST is done for scrapping.\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splinter method\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# ------constaint errors throught the notebook without the above method added each time scraping happends on a new page-----\n",
    "\n",
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "results = soup.find_all('div', class_='content_title')\n",
    "\n",
    "news_title = soup.find('div', class_='content_title').get_text()\n",
    "print(news_title)\n",
    "\n",
    "news = soup.find('div', class_='article_teaser_body').get_text()\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title=\"AI Is Helping Scientists Discover Fresh Craters on Mars\"\n",
    "news =\"It's the first time machine learning has been used to find previously unknown craters on the Red Planet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to check scraping progress\n",
    "print(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JPL Mars Space Images - Featured Image\n",
    "Visit the url for the Featured Space Image page here (https://spaceimages-mars.com/)\n",
    "\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "\n",
    "Make sure to find the image url to the full size .jpg image.\n",
    "Make sure to save a complete url string for this image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splinter method\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# ------constaint errors throught the notebook without the above method added each time scraping happends on a new page-----\n",
    "\n",
    "mars_url = 'https://spaceimages-mars.com/'\n",
    "browser.visit(mars_url)\n",
    "\n",
    "html_mars = browser.html\n",
    "\n",
    "soup = BeautifulSoup(html_mars, 'html.parser')\n",
    "\n",
    "featured_url = soup.find(\"img\", class_=\"headerimage fade-in\")[\"src\"]\n",
    "featured_image_url = f'https://spaceimages-mars.com/{featured_url}'\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_image_url= 'https://spaceimages-mars.com/image/featured/mars1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Facts\n",
    "\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "\n",
    "Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://galaxyfacts-mars.com/'\n",
    "tables = pd.read_html(url)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task asked for the planet so I asssumed mars. \n",
    "\n",
    "df = tables[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "cols[0]=\"Comparison\"\n",
    "cols[1] = \"Mars\"\n",
    "df.columns = cols\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_table = df.to_html()\n",
    "html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_table.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# splinter method\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "\n",
    "url_hem = 'https://marshemispheres.com/'\n",
    "browser.visit(url_hem)\n",
    "\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "results = soup.find_all('div', class_='item')\n",
    "image_url = soup.find(\"img\", class_=\"thumb\")[\"src\"]\n",
    "print(url_hem + image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating loop to get all of the page images and names \n",
    "\n",
    "hemisphere_image_link =[]\n",
    "\n",
    "for info in results: \n",
    "    title = info.find('h3').get_text()\n",
    "    page_img = info.find('a', class_='itemLink product-item')['href']\n",
    "    browser.visit(url_hem + page_img)\n",
    "    \n",
    "    page_img = browser.html\n",
    "    soup = BeautifulSoup(page_img, 'html.parser')\n",
    "    image_url = url_hem + soup.find('img', class_='thumb')['src']\n",
    "    hemisphere_image_link.append({'title' : title, 'img_url' : image_url})\n",
    "\n",
    "    # print(image_url)\n",
    "hem_data=dict({'title':title, 'img_url':image_url})\n",
    "hemisphere_image_link.append(hem_data)\n",
    "    \n",
    "    \n",
    "hemisphere_image_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splinter method\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "\n",
    "url_hem = 'https://marshemispheres.com/'\n",
    "browser.links.find_by_href('cerberus.html').click()\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "results = soup.find_all('a', target=\"_blank\", class_='thumb')['href'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splinter method\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# ---------------------------------------------SEE test notebook where the code executed correctly, still troubleshooting why the code is \n",
    "# not working in this notebook. Likely my extenstion is not reading the code correctly after a point---------------------------------------------------- \n",
    "\n",
    "# list to hold the images and titles.\n",
    "\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Code to retrieve the image urls and titles for each hemisphere.\n",
    "\n",
    "# First, we need to get a get a list of all of the hemispheres\n",
    "links = browser.find_by_css('a.product-item img')\n",
    "\n",
    "# Next, loop through those links, click the link, find the sample anchor, and return the href we want\n",
    "for i in range(len(links)):\n",
    "    hemisphere = {}\n",
    "    \n",
    "    # We have to find the elements on each loop to avoid a stale element exception\n",
    "    browser.find_by_css('a.product-item img')[i].click()\n",
    "    \n",
    "    #  Sample image anchor tag and extract the href\n",
    "    sample_elem = browser.links.find_by_text('Sample').first\n",
    "    hemisphere['img_url'] = sample_elem['href']\n",
    "    \n",
    "    # Get Hemisphere title\n",
    "    hemisphere['title'] = browser.find_by_css('h2.title').text\n",
    "    \n",
    "    # Append hemisphere object to list\n",
    "    hemisphere_image_urls.append(hemisphere)\n",
    "    \n",
    "    # Finally, we navigate backwards\n",
    "    browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "hemisphere_image_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('bootcamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abb041905d78b0e8d42a8ca89e8df410324f3397d57804e78a833def6a13175"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
